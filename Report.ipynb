{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerPuff Grills - Pattern Recognition - Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Group Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Channels**:\n",
    "We used WhatsApp as communication platform to organize meetings and to update each other with the current progress and status of our work. \n",
    "On GitHub we created some issues to assign tasks to the team members. \n",
    "We worked with different branches to develop independently and to finally merge everything into the master branch. \n",
    "We met weekly to plan and discuss the progress and the next steps.\n",
    "\n",
    "- **Planning**:\n",
    "For each task we discussed how we could split the work into subtasks and how we could assign them to the team members.\n",
    "We tried to estimate the effort for each subtask and their dependencies.\n",
    "Finally, we defined deadlines for each subtask to make sure that we will comply with the given submission date.\n",
    "\n",
    "- **Execution**: \n",
    "Depending on the subtask, we worked alone, in pairs or in a subgroup.\n",
    "In cases of dependencies we communicated the progress and briefed each other about the implemented solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is special about our solution**\n",
    "\n",
    "\n",
    "**Our approach:**\n",
    "- We implemented the SVM algorithm for three different kernels: linear, polynomial and RBF, using cross-validation and parameter tuning.\n",
    "- To get the best parameter combination, we created arrays for possible C-, gamma-, degree-values and used GridSearchCV from the sklearn library.\n",
    "\n",
    "\n",
    "**What worked, what didn’t work**\n",
    "- The computation of the best parameter combination never finished, not even on the cluster.\n",
    "- We could test the execution for the single kernels and with given parameters, but still, the training took ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is special about our solution**\n",
    "\n",
    "\n",
    "**Our approach:**\n",
    "We used the DeepDiva framework to train and test the network. To build the architecture of the MLP model, we got the inspiration from the already existing models in DeepDiva.\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is special about our solution**\n",
    "\n",
    "\n",
    "**Our approach:**\n",
    "We used the DeepDiva framework to train and test the network. In order to complete the provided CNN implementation we  \n",
    "1. investigated the shape (width and height) and number of input-channels of the images from the MNIST dataset with help of the appropriate Python-function\n",
    "2. calculated the number of output-channels and the kernel size that it fits to the predefined classifier-layer with the formula given in the lecture\n",
    "3. provided the final linear layer with the output size, which was given by the number of classes in the MNIST dataset\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "- Some group members didn’t manage to install or execute the DeepDiva framework\n",
    "- In case of a running DeepDiva framework it further took a while to get warm with the handling. The documentation is short and clear though and in the end the framework was straightforward to use.\n",
    "- The visualization with the integrated Tensorboard was really cool, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is special about our solution**\n",
    "\n",
    "**Our approach:**\n",
    "In order to fulfill this task we performed the following:\n",
    "1. We iterate over all keywords\n",
    "2. For every keyword, we get one corresponding train word\n",
    "3. We calculate the distance between this train word to every validation word by applying the “Distance Time Warping” algorithm, with the help of the “fastdtw” implementation (https://pypi.org/project/fastdtw/). For the DTW we extracted the following features: lower contour, upper contour, the number of black pixels between lower and upper contour, the number of black pixels between lower and upper contour, the number of black pixels and the number of black/white transitions\n",
    "4. We sort the distances and select the most similar words according to a given threshold\n",
    "5. Finally, we calculate the confusion matrix to get the accuracy and recall\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "- We had some problems with the keyword spotting task since the task description is unclear (for example, we did not really understand what the output of the task should be)\n",
    "- It was much harder to organize this task, since there were many dependencies and working in parallel was not really possible, in contrast to task 2\n",
    "- We didn’t implement the Precision-Recall-Curve to test the performance of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is special about our solution:**\n",
    "\n",
    "**Our approach:**\n",
    "We implemented the molecule Task using linear_sum_assignment from scipy.optimize which directly implements the Hungarian algorithm.\n",
    "1. We first translate all gxl files into python objects containing all the needed information, since we use them multiple times.\n",
    "2. We created the Dirac matrix according the literature and the scipy-documentation for the algorithm explained how to handle the results to calculate/approximate GED.\n",
    "3. Then we simply iterate through both sets and calculate the distance matrix using KNN (like in exercise 1)\n",
    "4. We tried to optimize the parameters Ce, Cn and k. With Cn=1 and k=3 we achieved good results and more testing didn’t improve the results further.\n",
    "\n",
    "\n",
    "**What worked, what didn’t work**\n",
    "- It took a while to understand hot to use gxl-files (but now it’s nice to know)\n",
    "- With the scipy-documentation the usage of the Hungarian algorithm and the calculation of the GED was straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion / Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make knowledge about programming mandatory\n",
    "- Maybe give cluster access from the beginning? It really helped us\n",
    "- Task 3: really unclear task description, hard to split because no work can be done in parallel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
