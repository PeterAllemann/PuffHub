{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerPuff Grills - Pattern Recognition FS2019 - Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Group Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Channels**:\n",
    "We used WhatsApp as communication platform to organize meetings and to update each other with the current progress and state of work.\n",
    "On GitHub we created some issues to assign tasks to the team members and worked on different branches to develop independently.\n",
    "We also had weekly meetings to plan, discuss the progress and the next steps.\n",
    "\n",
    "- **Planning**:\n",
    "For each task we discussed how we could split the work into subtasks and how we could assign them to the team members.\n",
    "We tried to estimate the effort for each subtask and their dependencies.\n",
    "Finally, we defined deadlines for each subtask to make sure that we will comply with the given submission date.\n",
    "\n",
    "- **Execution**: \n",
    "Depending on the subtask, we worked alone, in pairs or in a subgroup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "We splitted the subtasks in the group and worked in parallel. For Task b), c) and d) we used the DeepDiva framework to train and test the networks.\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "- Some group members didn’t manage to install or execute the DeepDiva framework at all.\n",
    "- In case of a running DeepDiva framework it further took a while to get warm with the handling. The documentation is short and clear though and in the end the framework was straightforward to use.\n",
    "- The visualization with the integrated Tensorboard was really cool, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "We implemented the SVM algorithm for three different kernels: linear, polynomial and RBF, using cross-validation and parameter tuning.\n",
    "To get the best parameter combination, we created arrays for possible C-, gamma-, degree-values and used GridSearchCV from the sklearn library.\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "The computation of the best parameter combination never finished, not even on the cluster.\n",
    "We could test the execution for the single kernels and with given parameters, but still, the training took ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "To build the architecture of the MLP model, we got the inspiration from the already existing models in DeepDiva.\n",
    "To get the best accuracy, we tried different value combinations for\n",
    "- learning rate (0.001, 0.003, 0.01, 0.03, 0.1)\n",
    "- hidden layers / number of neurons (10, 30, 50, 100, 500)\n",
    "- epochs (5, 10, 20)\n",
    "\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "We could observe that the bigger the hidden layer size and the epochs, the higher is the accuracy.\n",
    "We also observed that if the learning rate is small (e.g. 0.001) we have to train longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "In order to complete the provided CNN implementation we  \n",
    "1. investigated the shape (width and height) and number of input-channels of the images with the appropriate Python-function.\n",
    "2. calculated the number of output-channels and the kernel size that it fits to the predefined classifier-layer with the formula given in the lecture.\n",
    "3. provided the final linear layer with the output size, which was given by the number of classes in the MNIST dataset.\n",
    "Finally, we trained the CNN with different learning rates and different amount of epochs to find the best accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Permutated MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- MLP: we simply used the best performing parameters from task 2b).\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "After performing Task 2b) and 2c) it was straightforward to perform this task, also because we already knew the DeepDiva framework and we had some knowledge about the best performing parameters (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "In order to fulfill this task we performed the following:\n",
    "1. We iterate over all keywords.\n",
    "2. For every keyword, we get one corresponding train word.\n",
    "3. We calculate the distance between this train word to every validation word by applying the “Distance Time Warping” algorithm, with the help of the “fastdtw” implementation (https://pypi.org/project/fastdtw/). For the DTW we extracted the following features: lower contour, upper contour, the number of black pixels between lower and upper contour, the number of black pixels between lower and upper contour, the number of black pixels and the number of black/white transitions.\n",
    "4. We sort the distances and select the most similar words according to a given threshold.\n",
    "5. Finally, we calculate the confusion matrix to get the accuracy and recall.\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "- We had some problems with the keyword spotting task since the task description is unclear (for example, we did not really understand what the output of the task should be).\n",
    "- It was much harder to organize this task, since there were many dependencies and working in parallel was not really possible, in contrast to task 2.\n",
    "- We didn’t implement the Precision-Recall-Curve to test the performance of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "We implemented the molecule Task using linear_sum_assignment from scipy.optimize which directly implements the Hungarian algorithm.\n",
    "1. We first translated all gxl files into python objects containing all the needed information, since we use them multiple times.\n",
    "2. We created the Dirac matrix according the literature and the scipy-documentation for the algorithm explained how to handle the results to calculate/approximate GED.\n",
    "3. Then we simply iterated through both sets and calculated the distance matrix using knn (like in exercise 1).\n",
    "4. We tried to optimize the parameters Ce, Cn and k. With Ce=1, Cn=1 and k=3 we achieved good results and more testing didn’t improve the results further.\n",
    "\n",
    "**What worked, what didn’t work:**\n",
    "- It took a while to understand how to use gxl-files (but now it’s nice to know).\n",
    "- With the scipy-documentation the usage of the Hungarian algorithm and the calculation of the GED was straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion / Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make knowledge about programming mandatory.\n",
    "- Maybe give cluster access from the beginning? It really helped us.\n",
    "- Task 2: This was a very cool task, since it was easy to split the workload in the group and you could work in parallel. And we learned a lot about (neural) networks and the impact of the different parameters.\n",
    "- Task 3: Really unclear task description, hard to split because no work can be done in parallel.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
